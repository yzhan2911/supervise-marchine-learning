{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "_In this notebook, every question will be marked by a blue border, and answers should be provided in cells in a green border. All code-related answers are preceded by a #TODO._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Students (to fill in)\n",
    "\n",
    " - Zhang yu (group c) \n",
    " - Zeng yongjia (group c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "The objective of this lab is to dive into particular kind of neural network: the *Multi-Layer Perceptron* (MLP).\n",
    "\n",
    "To start, let us take the dataset from the previous lab (hydrodynamics of sailing boats) and use scikit-learn to train a MLP instead of our hand-made single perceptron.\n",
    "The code below is already complete and is meant to give you an idea of how to construct an MLP with scikit-learn. You can execute it, taking the time to understand the idea behind each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mgenfromtxt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myacht_hydrodynamics.data\u001b[39m\u001b[38;5;124m\"\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m dataset[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "import numpy as np\n",
    "dataset = np.genfromtxt(\"yacht_hydrodynamics.data\", delimiter='')\n",
    "X = dataset[:, :-1]\n",
    "Y = dataset[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocessing: scale input data \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      3\u001b[0m sc \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mfit_transform(X)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Preprocessing: scale input data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Split dataset into training and test set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, Y,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.20\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y,random_state=1, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define a multi-layer perceptron (MLP) network for regression\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneural_network\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLPRegressor\n\u001b[1;32m      3\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MLPRegressor(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# define the model, with default params\u001b[39;00m\n\u001b[1;32m      4\u001b[0m mlp\u001b[38;5;241m.\u001b[39mfit(x_train, y_train) \u001b[38;5;66;03m# train the MLP\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Define a multi-layer perceptron (MLP) network for regression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp = MLPRegressor(max_iter=3000, random_state=1) # define the model, with default params\n",
    "mlp.fit(x_train, y_train) # train the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain score: \u001b[39m\u001b[38;5;124m'\u001b[39m, mlp\u001b[38;5;241m.\u001b[39mscore(x_train, y_train))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest score:  \u001b[39m\u001b[38;5;124m'\u001b[39m, mlp\u001b[38;5;241m.\u001b[39mscore(x_test, y_test))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print('Train score: ', mlp.score(x_train, y_train))\n",
    "print('Test score:  ', mlp.score(x_test, y_test))\n",
    "plt.plot(mlp.loss_curve_)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot the results\u001b[39;00m\n\u001b[1;32m      2\u001b[0m num_samples_to_plot \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(y_test[\u001b[38;5;241m0\u001b[39m:num_samples_to_plot], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mro\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m yw \u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mpredict(x_test)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(yw[\u001b[38;5;241m0\u001b[39m:num_samples_to_plot], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbx\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mhat\u001b[39m\u001b[38;5;132;01m{y}\u001b[39;00m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot the results\n",
    "num_samples_to_plot = 20\n",
    "plt.plot(y_test[0:num_samples_to_plot], 'ro', label='y')\n",
    "yw = mlp.predict(x_test)\n",
    "plt.plot(yw[0:num_samples_to_plot], 'bx', label='$\\hat{y}$')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Examples\")\n",
    "plt.ylabel(\"f(examples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Analyzing the network\n",
    "\n",
    "Many details of the network are currently hidden as default parameters.\n",
    "\n",
    "Using the [documentation of the MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html), answer the following questions.\n",
    "<!-- Question Start -->\n",
    "<div style=\"border: 1px solid blue; padding: 20px;border-radius: 5px;\">\n",
    "    \n",
    "- What is the structure of the network?\n",
    "- What it is the algorithm used for training? Is there algorithm available that we mentioned during the courses?\n",
    "- How does the training algorithm decides to stop the training?\n",
    "</div>\n",
    "<!-- Question End -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- Answer Section Start -->\n",
    "<div style=\"border: 1px solid green; padding: 10px; margin-top: 10px; border-radius: 5px\">\n",
    "\n",
    "**Your answer here:**\n",
    "\n",
    "    1 Le reseau utilise est un perceptron multicouche(MLP)configuré par les paramètres par deflats de la bibliotheque de scikit-learn.Et nous utilisons la configuration default, qui est donc une seule couche cachee avec 100 neurones.\n",
    "    2 L'algo pour l'entrainement que nous utilisons est celui par default dans MLPRegressor dans scikit-learn,qui est ‘adam’ .Dans le cours on a étudié l'algorithme de Stochastic gradient descent,cet algorithme est aussi disponible dans scikit-learn,qui est 'sgd'.\n",
    "    3 L'entrainement sera arreté quand l'algorithme converge,ou quand algo a exécuté max-iter fois, ici c'est 3000. Cela peut garantir l'entrainnement ne se poursuit pas indéfiniment. ici comme on n'a pas modifié early stopping, donc cet algo ne va pas arrette en avance, mais normalement quand la perte est converge ,ça veut dire que la perte d'entraînement atteint un seuil acceptable. Cela indique que le modèle a convergé vers une solution acceptable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>\n",
    "<!-- Answer Section End -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Onto a more challenging dataset: house prices\n",
    "\n",
    "For the rest of this lab, we will use the (more challenging) [California Housing Prices dataset](https://www.kaggle.com/datasets/camnugent/california-housing-prices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean all previously defined variables for the sailing boats\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Import the required modules\"\"\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_california_housing\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shuffle\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "\"\"\"Import the required modules\"\"\"\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "cal_housing = fetch_california_housing()\n",
    "print(f\"dataset type : {type(cal_housing)}\")\n",
    "print(f\"number of data : {len(cal_housing.data)}\")\n",
    "X_all = pd.DataFrame(cal_housing.data,columns=cal_housing.feature_names)\n",
    "y_all = pd.DataFrame(cal_housing.target,columns=[\"target\"])\n",
    "\n",
    "X_all, y_all = shuffle(X_all, y_all, random_state=1)\n",
    "\n",
    "display(X_all.head(10)) # print the first 10 values\n",
    "display(y_all.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Note that each row of the dataset represents a **group of houses** (one district). The `target` variable denotes the average house value in units of 100.000 USD. Median Income is per 10.000 USD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "The dataset consists of 20,000 datas. We first extract the last 5,000 for test samples, which we will use later.\n",
    "\n",
    "For training and validation, we will use a subset consisting of only 2,000 datas to speed up computations.\n",
    "\n",
    "<!-- Question Start -->\n",
    "<div style=\"border: 1px solid blue; padding: 20px;border-radius: 5px;\">\n",
    "    \n",
    "- Split those 2000 remaining dataset between a training set and a validation set (see usage of `train_test_split` function earlier)\n",
    "- Why did you choose this partition?\n",
    "- What is the purpose of each subset (train, validation, test) ?\n",
    "\n",
    "</div>\n",
    "<!-- Question End -->\n",
    "\n",
    "\n",
    "Please use the conventional names `X_train`, `X_val`, `y_train` and `y_val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the last N samples for test (for later use)\n",
    "num_test_samples = 5000\n",
    "X_test, y_test = X_all[-num_test_samples:], y_all[-num_test_samples:]\n",
    "\n",
    "# only use the first N samples to limit training time\n",
    "num_samples = 2000\n",
    "X, y = X_all[:num_samples], y_all[:num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m X_train,X_val,y_train,y_val \u001b[38;5;241m=\u001b[39m train_test_split(X,y,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Ici, test_size=0.05 signifie que 5% des données (100 données) seront utilisées pour la validation,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# et les 95% restants (1900 données) pour l'entrainement.\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_val,y_train,y_val = train_test_split(X,y,random_state=1, test_size=0.05)\n",
    "# Ici, test_size=0.05 signifie que 5% des données (100 données) seront utilisées pour la validation,\n",
    "# et les 95% restants (1900 données) pour l'entrainement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- Answer Section Start -->\n",
    "<div style=\"border: 1px solid green; padding: 10px; margin-top: 10px; border-radius: 5px\">\n",
    "\n",
    "**Your answer here:**\n",
    "    \n",
    "    Question 2:\n",
    "    nous avons choisi une petite portion pour la validation pour maximiser la quantite de donées disponbles pour l\"entrainement et pour éviter de overfitting.\n",
    "    \n",
    "    Question 3: \n",
    "    - train: Cet ensemble est utilisé pour entraîner le modèle. L'objectif est de permettre au modèle d'apprendre la structure et les tendances présentes dans les données.\n",
    "    \n",
    "    - validation : Utilisé pour évaluer le modèle pendant l'entraînement. Cela permet de vérifier si cet ensemble pour faire des ajustements fins sur le modèle, pour qu'il puisse mieux performer, un peu comme on ajusterait sa méthode d'étude avant un grand examen.\n",
    "    \n",
    "    - test : Après que le modèle ait été ajusté et entraîné ,cet ensemble fournit une évaluation finale . Il montre comment le modèle va se comporter dans des situations réelles, avec de nouvelles données qu'il n'a jamais vues. Cela permet de s'assurer que le modèle est prêt à être utilisé dans le monde réel, ou de comparer sa performance à celle d'autres modèles pour voir lequel est le meilleur.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>\n",
    "<!-- Answer Section End -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Scaling the input data\n",
    "\n",
    "\n",
    "A step of **scaling** of the data is often useful to ensure that all input data centered on 0 and with a fixed variance.\n",
    "\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). The function `StandardScaler` from `sklearn.preprocessing` computes the standard score of a sample as:\n",
    "\n",
    "```\n",
    "z = (x - u) / s\n",
    "```\n",
    "\n",
    "where `u` is the mean of the training samples, and `s` is the standard deviation of the training samples.\n",
    "\n",
    "<!-- Question Start -->\n",
    "<div style=\"border: 1px solid blue; padding: 20px;border-radius: 5px;\">\n",
    "    \n",
    "- Using the `StandardScaler`, first fit this scaler on your training dataset (`X_train`), then use this fitted scaler to transform the training dataset, the validation dataset (`X_val`), and the test dataset (`X_test`).\n",
    "\n",
    "\n",
    "- Why is it important to fit the scaler only on the training data and not on the entire dataset or separately on each dataset?\n",
    "\n",
    "</div>\n",
    "<!-- Question End -->\n",
    "\n",
    "[Documentation of standard scaler in scikit learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train= sc.fit_transform(X_train)\n",
    "X_val= sc.transform(X_val)\n",
    "X_test=sc.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- Answer Section Start -->\n",
    "<div style=\"border: 1px solid green; padding: 10px; margin-top: 10px; border-radius: 5px\">\n",
    "\n",
    "**Your answer here:**\n",
    "\n",
    "     Eviter la fuite de données : Ajuster le scaler uniquement sur les données d'entrainement empêche d'utiliser des informations provenant des ensembles de validation et de test, ce qui pourrait altérer les évaluations de la performance du modèle.\n",
    "    Respecter le processus d'apprentissage : Le modèle doit simuler des conditions réelles où il ne connaît pas les nouvelles données à l'avance. Ajuster le scaler sur l'entraînement assure que le modèle est évalué de manière équitable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>\n",
    "<!-- Answer Section End -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Overfitting\n",
    "\n",
    "In this part, we are only interested in maximizing the **train score**, i.e., having the network memorize the training examples as well as possible. While doing this, you should (1) remain within two minutes of training time, and (2) obtain a score that is greater than 0.90.\n",
    "\n",
    "<!-- Question Start -->\n",
    "<div style=\"border: 1px solid blue; padding: 20px;border-radius: 5px;\">\n",
    "    \n",
    "- Propose a parameterization of the network (number of neurons per layer, number of layers, epochs, learning rates) that will maximize the train score (without considering the test score). Ensure that you disable any form of internal validation checks such as early stopping to promote overfitting.\n",
    "\n",
    "- Is the **validation** score substantially smaller than the **train** score (indicator of overfitting) ?\n",
    "- Explain how the parameters you chose allow the learned model to overfit.\n",
    "</div>\n",
    "<!-- Question End -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/insa/anaconda/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.9585888747786508\n",
      "Test score:   0.5925719726198875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(100, 80,60,40), # 4 couches avec 100,80,60,40 neurones\n",
    "                   activation='relu',           \n",
    "                   solver='adam',                 \n",
    "                   max_iter=3000,                \n",
    "                   random_state=1) \n",
    "mlp.fit(X_train, y_train) \n",
    "train_score=mlp.score(X_train, y_train)\n",
    "test_score= mlp.score(X_val, y_val)\n",
    "print('Train score: ',train_score )\n",
    "print('Test score:  ',test_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- Answer Section Start -->\n",
    "<div style=\"border: 1px solid green; padding: 10px; margin-top: 10px; border-radius: 5px\">\n",
    "\n",
    "**Your answer here:**\n",
    "\n",
    "  - notre validation score est plus petit que celui de train \n",
    "\n",
    "    - hidden_layer_sizes=(100, 80,60,40)  \n",
    "    Un grand nombre de neurones et de couches accroît la complexité du modèle, ce qui lui permet d'apprendre des details des données d'entrainement.\n",
    "   \n",
    "    - activation='relu',          \n",
    "    La fonction d'activation ReLU favorise un apprentissage rapide, mais peut également contribuer à un overfiting, surtout lorsque le modèle est profond et complexe.\n",
    "   \n",
    "    -solver='adam', max_iter=3000,             \n",
    "    L'optimiseur Adam, associé à un nombre élevé d'itérations, permet au modèle de minimiser les erreurs sur l'entrainement plus efficace, en augmentant aussi le risque de overfiting.\n",
    "\n",
    "\n",
    "</div>\n",
    "<!-- Answer Section End -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "In this section, we are now interested in maximizing the ability of the network to predict the value of unseen examples, i.e., maximizing the **validation** score.\n",
    "You should experiment with the possible parameters of the network in order to obtain a good test score, ideally with a small learning time.\n",
    "\n",
    "Parameters to vary:\n",
    "\n",
    "- number and size of the hidden layers\n",
    "- activation function\n",
    "- stopping conditions\n",
    "- maximum number of iterations\n",
    "- initial learning rate value\n",
    "\n",
    "Results to present for the tested configurations:\n",
    "\n",
    "- Train/val score\n",
    "- training time\n",
    "\n",
    "<!-- Question Start -->\n",
    "<div style=\"border: 1px solid blue; padding: 20px;border-radius: 5px;\">\n",
    "Present in a table the various parameters tested and the associated results. \n",
    "</div>\n",
    "<!-- Question End -->\n",
    "\n",
    "You can find a cell in the notebook a code snippet that will allow you to plot tables from python structure.\n",
    "Be methodical in the way your run your experiments and collect data. For each run, you should record the parameters and results into an external data structure.\n",
    "\n",
    "(Note that, while we encourage you to explore the solution space manually, there are existing methods in scikit-learn and other learning framework to automate this step as well, e.g., [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/insa/anaconda/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.8728644040732881\n",
      "Validation score:   0.7430546827401804\n",
      "train time:  1.998474359512329\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "import time\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(100,200,300,200,100), # 4 couches avec 100,80,60,40 neurones\n",
    "                   activation='relu',\n",
    "                   early_stopping=True,\n",
    "                   solver='adam',                 \n",
    "                   max_iter=3000,\n",
    "                   alpha=0.0003,\n",
    "                   learning_rate_init=0.001,\n",
    "                   random_state=1)\n",
    "start_time=time.time()\n",
    "mlp.fit(X_train, y_train)\n",
    "end_time=time.time()\n",
    "train_score=mlp.score(X_train, y_train)\n",
    "test_score= mlp.score(X_val, y_val)\n",
    "duree=end_time-start_time\n",
    "print('Train score: ',train_score )\n",
    "print('Test score:  ',test_score)\n",
    "print('train time: ',duree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>max_iter</th>\n",
       "      <th>test_score</th>\n",
       "      <th>early_stopping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tanh</td>\n",
       "      <td>200</td>\n",
       "      <td>0.91</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>500</td>\n",
       "      <td>0.87</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  activation max_iter  test_score early_stopping\n",
       "1       tanh      200        0.91          False\n",
       "0       relu      500        0.87              -"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code snippet to display a nice table in jupyter notebooks  (remove from report)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = []\n",
    "data.append({'activation': 'relu', 'max_iter': '500', 'test_score': 0.87})\n",
    "data.append({'activation': 'tanh', 'max_iter': '200', 'early_stopping': False, 'test_score': 0.91})\n",
    "######???????\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "table = table.replace(np.nan, '-')\n",
    "table = table.sort_values(by='test_score', ascending=False)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Evaluation\n",
    "<!-- Question Start -->\n",
    "<div style=\"border: 1px solid blue; padding: 20px;border-radius: 5px;\">\n",
    "    \n",
    "- From your experiments, what seems to be the best model (i.e. set of parameters) for predicting the value of a house?\n",
    "- Evaluate the score of your model on the test set that was not used for training nor for model selection.\n",
    "- Train a model using your optimal parameters on the initial 15,000 data points. Evaluate the performance using the test set. What are your thoughts on the amount of data used? Do you believe the time spent is worthwhile in terms of the improvement in performance?\n",
    "</div>\n",
    "<!-- Question End -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- Answer Section Start -->\n",
    "<div style=\"border: 1px solid green; padding: 10px; margin-top: 10px; border-radius: 5px\">\n",
    "\n",
    "**Your answer here:**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>\n",
    "<!-- Answer Section End -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
